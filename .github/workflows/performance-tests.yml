name: Performance Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - database
          - scanning
          - ui
          - matching
      compare_baseline:
        description: 'Compare with baseline'
        required: false
        default: true
        type: boolean

env:
  PYTHON_VERSION: '3.11'

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Relaxed timeout for performance tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Get full history for baseline comparison

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark psutil  # Ensure performance dependencies

    - name: Create performance test directories
      run: |
        mkdir -p tests/performance/results
        mkdir -p tests/performance/data

    - name: Check system resources
      run: |
        python -c "
import psutil
import platform
print(f'CPU: {psutil.cpu_count(logical=False)} cores, {psutil.cpu_count()} logical')
print(f'Memory: {psutil.virtual_memory().total // (1024**3)} GB')
print(f'Disk: {psutil.disk_usage(\"/\").total // (1024**3)} GB')
print(f'Python: {platform.python_version()}')
print(f'OS: {platform.system()} {platform.release()}')
        "

    - name: Download baseline data (if exists)
      continue-on-error: true
      run: |
        # Try to download baseline from previous successful run
        if [[ "${{ github.event_name }}" == "push" && "${{ github.ref }}" == "refs/heads/main" ]]; then
          echo "Skipping baseline download for main branch push"
        else
          curl -L -o tests/performance/results/baseline_benchmark.json \
            "https://api.github.com/repos/${{ github.repository }}/actions/artifacts/baseline_benchmark" || true
        fi

    - name: Run performance benchmarks
      run: |
        if [[ "${{ github.event.inputs.suite }}" == "database" ]]; then
          python tests/performance/runner.py --suite database --no-compare
        elif [[ "${{ github.event.inputs.suite }}" == "scanning" ]]; then
          python tests/performance/runner.py --suite scanning --no-compare
        elif [[ "${{ github.event.inputs.suite }}" == "ui" ]]; then
          python tests/performance/runner.py --suite ui --no-compare
        elif [[ "${{ github.event.inputs.suite }}" == "matching" ]]; then
          python tests/performance/runner.py --suite matching --no-compare
        else
          python tests/performance/runner.py --no-compare
        fi

    - name: Generate performance report
      run: |
        python tests/performance/runner.py --report

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results-${{ github.run_number }}
        path: |
          tests/performance/results/
          tests/performance/performance_report.md
        retention-days: 30

    - name: Upload benchmark JSON
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-data-${{ github.run_number }}
        path: tests/performance/results/latest_benchmark.json
        retention-days: 90

    - name: Check for performance regressions
      run: |
        # Check if regressions were detected in the output
        if grep -q "Performance Regressions Detected" tests/performance/results/benchmark_*.txt 2>/dev/null; then
          echo "âš ï¸ Performance regressions detected!"
          echo "Check the performance report for details."
          exit 1
        else
          echo "âœ… No performance regressions detected"
        fi

    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = './tests/performance/performance_report.md';
          
          if (fs.existsSync(path)) {
            const report = fs.readFileSync(path, 'utf8');
            
            // Create comment with performance summary
            const comment = `
## ðŸš€ Performance Test Results

${report}

---
*Results from run #${{ github.run_number }}*
            `;
            
            // Find existing performance comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(c => 
              c.body.includes('Performance Test Results') && 
              c.user.type === 'Bot'
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
          }

    - name: Update baseline (main branch only)
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        # Set latest results as baseline for main branch
        python tests/performance/runner.py --set-baseline
        echo "âœ… Performance baseline updated"

  performance-summary:
    runs-on: ubuntu-latest
    needs: performance-tests
    if: always()
    
    steps:
    - name: Download performance results
      uses: actions/download-artifact@v3
      with:
        name: performance-results-${{ github.run_number }}
        path: performance-results/

    - name: Performance Summary
      run: |
        echo "## ðŸš€ Performance Test Summary" >> $GITHUB_STEP_SUMMARY
        
        if [ -f performance-results/performance_report.md ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          cat performance-results/performance_report.md >> $GITHUB_STEP_SUMMARY
        else
          echo "No performance report found" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f performance-results/results/latest_benchmark.json ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“Š Benchmark Data" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark data available in artifacts" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Check for failures
      if: needs.performance-tests.result == 'failure'
      run: |
        echo "âŒ Performance tests failed!" >> $GITHUB_STEP_SUMMARY
        echo "Check the job logs for details." >> $GITHUB_STEP_SUMMARY
        exit 1